\documentclass[a4paper,12pt]{report}

\usepackage{float}

\usepackage[brazilian]{babel}
\usepackage[T1]{fontenc}
\usepackage[indent=20pt]{parskip}

\usepackage{amsmath}

\usepackage{biblatex}
\usepackage{hyperref}
\addbibresource{refs.bib}

\title{Projeto \RN{2} --- Operações sobre conjuntos (sets)}
\date{\today}

\begin{document}

\maketitle

\section*{Introdução}

Conjuntos podem ser compreeendidos como uma coleção abstrata de elementos
que não possui uma ordem específica e permite operações como inserção,
remoção, união e interseção, bem como verificação de pertencimento. Nesse
projeto foi implementado um conjunto capaz de guardar como elementos números
inteiros, sendo possível ao usuário escolher uma dentre duas estruturas de 
dados para servir de implementação do conjunto. 

\section*{Implementação}

Para representar os conjuntos, decidimos usar árvores binárias de busca (ABBs)
do tipo AVL e LLRB (left-leaning red-black tree). Dessa forma, as árvores se
mantém balanceadas de acordo com as regras de balanceamento para cada tipo de
árvore, garantido que a altura das árvores será sempre limitada superiormente
por um certo fator (um múltiplo constante de $\log n$, sendo $n$ a quantidade de 
elementos), permitindo a implementação das operações de modo eficiente.

Para manter o balanceamento das árvores AVL, as operações rotação simples 
para a esquerda/direita e rotação dupla direita-esquerda/esquerda-direita
são utilizadas. Já para a LLRB, as operações usadas são rotação para a 
esquerda/direita e inversão de cores (na inserção e remoção), bem como 
propagação de cor para a esquerda/direita (apenas na remoção).

Além das operações mais convencionais em ABBs, também foram implementadas
as operações juntar/catenar (\textit{join}/\textit{catenate}) e separar 
(\textit{split}), essas duas apenas para a árvore AVL, visto que adaptar 
a implementação abordada para a LLRB seria um processo não trivial, 
envolvendo decisões de projeto mais complexas, como decidir se um nó deveria 
ou não armazenar sua altura negra. As operações mencionadas permitem a 
implementação da operação união de modo mais eficiente comparado à implementação 
``ingênua'' de união entre ABBs \parencite{bfs16}.

Um detalhe importante para a nossa implementação é que os valores armazenados
em cada nó da árvore correspondem diretamente às chaves usadas para ordenação
da ABB. Caso não houvesse essa correspondência, seria necessário adaptar os
algoritmos \textit{join} e \textit{split} apresentados. Além disso, nossa 
implementação assume e mantém a invariante de unicidade das chaves.

Considerando as estruturas de dados estudadas, uma outra possível estrutura
de dados para implementação do conjunto seria a lista. Embora uma lista sequencial
ordenada permitisse que as operações união e interseção fossem realizadas 
em $O(n)$ e que a busca fosse realizada em $O(\log n)$, sendo $n$ o tamanho 
do maior conjunto, isso seria em detrimento das operações inserção e remoção,
que seriam $O(n)$ no pior caso, devido aos deslocamentos. Já na lista encadeada
e na lista sequencial não ordenada a busca seria $O(n)$, enquanto a operação 
interseção seria $O(n^2)$ no pior caso. Portanto, uma ABB balanceada é a melhor 
estrutura de dados dentre as que foram estudadas para a implementação das operações
de um conjunto.

\section*{Análise de Complexidade}

\subsection*{Árvores binárias de busca}

Árvores AVL e rubro-negras são árvores binárias de busca que se mantém 
balanceadas de acordo com determinadas regras, logo, a complexidade
de tempo para a operação busca será a mesma em ambas. A operação busca é
usada para implementar verificação de pertencimento em um conjunto. Uma 
implementação convencional, recursiva, de busca em ABB consiste de uma
verificação para tratar o caso onde se chega a um nó nulo ou um nó com o valor
desejado. Caso nenhum desses dois casos seja atingido, a busca prossegue pela
esquerda ou pela direita. No pior caso, a busca terminará em um nó nulo, sem
encontrar o valor desejado. Portanto, podemos montar a seguinte relação de 
recorrência para descrever a complexidade dessa operação:

\[
                      T(n) = T\left(\frac{n}{2}\right) + O(1)
\]

Pelo Teorema Mestre, sabemos que $T(n) = O(\log n)$.

Além disso, as operações inserção e remoção também podem ser analisadas em suas
formas mais simples, apenas para ABBs, para que depois sejam abordadas em mais
detalhes considerando as operações de balanceamento das árvores AVL e LLRB.

A operação inserção em uma ABB consiste em realizar uma busca pelo valor a ser
inserido. Caso o valor já exista na árvore, não há nada a ser feito. Caso não 
exista, deve ser inserido na posição atual. A inserção é, novamente, uma simples
manipulação de ponteiros; se o local de inserção já for conhecido de antemão,
a inserção pode ser realizada em $O(1)$. Portanto, a inserção tem complexidade
de tempo de $O(\log n)$.

Por sua vez, a operação remoção consiste em buscar o valor a ser removido. Se o
valor não for encontrado, não há nada a ser feito (caso \Rn{1}). Se o nó 
correspondente tiver um filho, esse nó é removido da árvore e substituído por 
seu filho (caso \Rn{2}). Se tiver dois filhos, devemos substituí-lo por um 
sucessor na árvore, seja esse o menor da subárvore direita ou o maior da subárvore 
esquerda (caso \Rn{3}).

Nos casos \Rn{1} e \Rn{2}, as operações de remoção têm complexidade de tempo constante 
ao se fixar a posição de remoção. No caso \Rn{3}, podemos notar que a substituição
pelo sucessor se dá também em $O(1)$ e que a soma da profundidade do nó a ser removido
com a altura do seu sucessor na subárvore escolhida deve ser igual à altura da árvore
original. Portanto, em todos os casos, a complexidade da operação remoção é $O(\log n)$.

Também adicionamos a operação clonar, que clona uma ABB em $O(n)$, recursivamente, 
apresentando a seguinte relação de recorrência:

\[
                      T(n) = 2T\left(\frac{n}{2}\right) + O(1)
\]

A operação interseção entre conjuntos foi implementada de forma ``ingênua'', pois 
essa forma é mais eficiente que o método abordado em \cite{bfs16} quando 
devemos preservar os conjuntos originais: sejam as árvores originais de tamanho $n$ 
e $m$, com $n > m$. Percorremos a árvore de menor tamanho, verificando se cada 
elemento existe na outra árvore e, se existir, o inserimos na árvore que irá guardar 
o resultado da interseção. Evidentemente, a complexidade de tempo dessa operação é 
$O(m (\log{n} + \log{m})) = O(m \log{n \cdot m})$ no pior caso, onde o termo $m$ 
corresponde à complexidade do percurso da árvore menor, $\log n$ à busca na árvore 
maior e $\log m$ à inserção na árvore resultante.

A operação união possui uma implementação otimizada para duas árvores do tipo AVL,
como mencionado a seguir. No entanto, se as duas árvores envolvidas na união não
forem ambas do tipo AVL, uma implementação ``ingênua'' é usada: a árvore de maior
tamanho é clonada e a árvore de menor tamanho é percorrida, tendo seus elementos
inseridos na árvore resultante. A complexidade de tempo dessa operação é da ordem 
de $O(n + m \log{n + m})$. 

\subsection*{Árvores AVL}

A operação rotação esquerda (\texttt{avl\_rotate\_left}) apresenta complexidade de 
tempo da ordem de $O(1)$, uma vez que é composta de apenas duas atribuições simples
de ponteiros e mais duas atribuições para atualizar a altura dos nós envolvidos
na rotação. A operação rotação direita é análoga (simétrica), enquanto as rotações
duplas são apenas duas rotações simples aplicadas em sequência. Portanto, a
complexidade de tempo de todas essas operações é $O(1)$.

A operação inserção em AVL consiste de inserção em uma ABB convencional, porém com
cálculo da altura e do fator de balanceamento do nó atual, seguido de rotações, na
volta da recursão, se for necessário, para corrigir casos de desbalanceamento.
As verificações e cálculos realizados são efetuados em $O(1)$, logo a complexidade
de inserção em árvores AVL é $O(\log n)$.

A remoção em árvores AVL é idêntica à remoção em ABBs, adotando o mesmo procedimento 
usado na inserção para corrigir desbalanceamentos na volta da recursão. 
Logo, a sua complexidade é $O(\log n)$.

Além disso, mencionaremos as operações \textit{join} e \textit{split}, que foram 
implementadas para a árvore AVL. Essas operações permitem a implementação da operação 
união entre duas árvores em $O(n)$ em vez de $O(n \log n)$. Ao passo que \cite{bfs16}
encontra um limitante superior ainda melhor, não iremos explorar essa possibilidade, 
uma vez que as técnicas usadas para análise de complexidade necessárias para obtenção 
desses limitantes melhores são muito complexas para serem abordadas nesse relatório. 
Além disso, outros fatores envolvendo decisões de projeto impedem esses limitantes 
superiores melhores de serem úteis na prática: como as operações \textit{join} e 
\textit{split} são destrutivas, devemos clonar as árvores antes de aplicar essas operações. 
Também devemos percorrer a árvore após realizar uma união para calcular seu tamanho. 
Dessa forma, a melhor complexidade que poderia ser obtida é $O(n)$.

A operação \textit{join} é definida da seguinte forma: sejam $T_1$ e $T_2$ ABBs e $k$ uma 
chave tal que $T_1 < k < T_2$. Ou seja, todos os valores em $T_1$ são menores do que $k$ 
e todos os valores em $T_2$ são maiores do que $k$. $\textrm{join}(T_1, k, T2)$ retorna uma 
árvore $T = T_1 \cup \{k\} \cup T_2$. Sua implementação é trivial em uma ABB convencional, 
porém se torna um pouco mais complicada quando se deseja que a árvore resultante se mantenha 
balanceada. A ideia por trás é percorrer a árvore de maior altura dentre $T_1$ e $T_2$, na 
direção em que as chaves se ``aproximam'' de $k$. Ao encontrar um nó, digamos, $v$, nessa árvore,
que possui altura $h + 1$, onde $h$ é a altura da menor árvore, podemos inserir um nó $k$ 
como filho desse nó encontrado, tornando o filho original de $v$ um dos filhos de $k$ e a 
raiz da outra árvore o outro filho de $k$. Após essa inserção, ajustes necessários para 
manter o balanceamento da árvore são feitos na volta da recursão, em $O(1)$ cada um.

Uma vez que a operação \textit{join} foi implementada recursivamente, podemos montar a seguinte 
relação de recorrência, considerando que as duas árvores envolvidas têm tamanho $n$ e que o
caso base se daria, no pior caso, após ter percorrido uma altura proporcional à altura da 
maior árvore, o que não é verdade, porém apenas piora a complexidade obtida, para simplificar 
a análise:

\[
                               T(n) = T\left(\frac{n}{2}\right) + O(1)
\]

Novamente, obtemos pelo Teorema Mestre que a complexidade de tempo da operação \textit{join} é 
$O(\log n)$.

Por sua vez, a operação \textit{split} é definida da seguinte forma: $\textrm{split}(T, k)$ 
retorna duas árvores, $T_1$ e $T_2$, onde $T_1 < k < T_2$, além de indicar se $k$ estava presente 
em $T$ ou não. A ideia por trás é realizar uma busca por $k$ em $T$, recursivamente. Na volta da 
recursão, teremos o resultado da operação \textit{split} no filho do nó atual na direção da busca. 
Digamos que esse resultado é composto de árvores $T_a$ e $T_b$ tais que $T_a < k < T_b$, de forma 
que o filho do nó atual que não foi analisado corresponde à raiz de uma árvore $T_c$ e que o valor 
do nó atual é dado por $A$. Se a busca seguiu pela esquerda, então $T_c > k$, logo devemos realizar 
a operação $\textrm{join}(T_b, A, T_c)$, guardando o resultado dessa operação como o novo $T_b$. 
Analogamente, se a busca seguiu pela direita, então $T_c < k$, logo devemos atualizar $T_a$ com o 
resultado de $\textrm{join}(T_c, A, T_a)$. O caso base da operação \textit{split} corresponde a um 
nó nulo ou nó com o valor $k$.

Dada a implementação da operação \textit{split}, é possível montar a seguinte relação de recorrência:

\[
                             T(n) = T\left(\frac{n}{2}\right) + O(\log n)
\]

Essa relação não pode ser solucionada usando o Teorema Mestre, no entanto, é possível solucioná-la
de forma aproximada pelo método da substituição, trocando $O(\log n)$ por $\log_2 n$: tome, por exemplo,
$T(2^3) = T(2^2) + \log_2 2^3 = T(2) + \log_2 2^2 + \log_2 2^3 = T(1) + \log_2 2 + \log_2 2^2 + \log_2 2^3$.
Assumindo que $T(1) = 0$, podemos deduzir que:

\[
           T(n) = \sum_{i=1}^{\log_2 n} i = \frac{\log_2 n \cdot (\log_2 n + 1)}{2} = O(\log^{2} n)
\]

Podemos verificar essa igualdade realizando a substituição na relação de recorrência, assumindo 
$\log$ na base $2$:

%\[
\begin{align*}
 T(n) &= \frac{\log{\hspace{-2pt}\left(\frac{n}{2}\right)} \cdot \left(\log{\hspace{-2pt}\left(\frac{n}{2}\right)} + 1\right)}{2} + \log n \\
	  &= \frac{\log^2\hspace{-2pt}\left(\frac{n}{2}\right)}{2} + \frac{\log{\hspace{-2pt}\left(\frac{n}{2}\right)}}{2} + \log n \\
      &= \frac{(\log n - \log 2)^2}{2} + \frac{\log n - \log 2}{2} + \log n \\
      &= \frac{\log^2 n - 2\log n + 1}{2} + \frac{\log n - 1}{2} + \frac{2\log n}{2} \\
      &= \frac{\log^2 n + \log n}{2}
      = \frac{\log n \cdot \left(\log n + 1\right)}{2}
\end{align*}
%\]

Portanto, a complexidade de tempo da operação \textit{split} é da ordem de $O(\log^2 n)$.

Por sua vez, a operação união recebe duas ABBs arbitrárias, $T_1$ e $T_2$, e retorna 
$T_1 \cup T_2$, destruindo $T_1$ e $T_2$. A ideia por trás é aplicar a operação 
\textit{split} em $T_1$, sendo $k$ dado pelo valor da raiz de $T_2$. Assim, obtemos 
duas árvores, $T_a$ e $T_b$, sendo $T_a$ os elementos de $T_1$ menores que $k$ e $T_b$ 
os elementos de $T_1$ maiores que $k$. Aplicamos então, recursivamente, a união entre 
$T_a$ e o filho esquerdo da raiz de $T_2$, bem como a união entre $T_b$ e o filho direito 
de $T_2$, obtendo, assim, duas árvores que podem ser juntadas usando a operação 
\textit{join}. Como não queremos que $T_1$ e $T_2$ sejam destruídas na implementação 
do conjunto, como já mencionado, devemos clonar as árvores antes de aplicar essa operação. 

O trabalho realizado pela operação união é proporcional a $T(n)$, dada pela seguinte 
relação de recorrência, onde o primeiro termo corresponde às chamadas recursivas,
o segundo termo à operação \textit{split} e o terceiro à operação \textit{join}.

\[
            T(n) = 2T\left(\frac{n}{2}\right) + O(\log^2 n) + O(\log n) 
                 = 2T\left(\frac{n}{2}\right) + O(\log^2 n)
\]

Essa relação de recorrência pode ser resolvida pelo Teorema Mestre, obtendo $O(n)$ para
complexidade da operação união entre duas árvores AVL, sendo $n$ o tamanho da maior árvore.
Portanto, levando em consideração o uso da operação clonar, que é $O(n)$, é possível 
realizar união entre dois conjuntos em $O(n)$.

\subsection*{Árvores LLRB}

A operação rotação esquerda em LLRBs também consiste de atribuições simples de ponteiros,
adicionalmente realizando atribuições para as cores dos nós para corrigir as cores após a
rotação. Esse procedimento é realizado em $O(1)$. A rotação direita é análoga. A operação
inversão de cor consiste de três verificações e três atribuições simples, sendo também da
ordem de $O(1)$.

A inserção em LLRBs é idêntica à inserção em ABBs, com a adição de três verificações
para corrigir casos que violem as regras de balanceamento da LLRB, portanto apresenta
a mesma complexidade de tempo. Por sua vez, a remoção possui verificações adicionais 
para o caso em que a busca prossegue para a direita ou para a esquerda, uma vez que é
necessário ``propagar'' as arestas vermelhas para o nó folha a ser removido. Essas 
verificações e ajustes, novamente, são da ordem de $O(1)$.

\subsection*{Detalhes}

É importante ressaltar que há algumas diferenças mais sutis entre a árvore AVL e a LLRB,
que não são refletidas nas complexidades de tempo de suas operações: a altura máxima que
uma árvore AVL pode atingir é $\log_{\varphi} n \approx 1{,}44 \log_2 n$, enquanto 
a altura máxima para uma árvore LLRB é $2 \log_2 n$. Ou seja, a árvore AVL é uma 
opção melhor que árvores RB para casos em que buscas são realizadas frequentemente.

\printbibliography

\end{document}
